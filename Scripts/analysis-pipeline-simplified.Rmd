---
title: "analysis-pipeline-simplified"
output: html_document
date: "2025-12-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(brms)
library(lme4)

target_prompt <- "Well, the"


test_df  = read_csv('../Data/attested_binoms_test.csv')
train_df = read_csv('../Data/attested_binoms_train.csv')


# all_human_data = nonce_data
all_human_data = read_csv('../Human Data and Analyses/all_human_data.csv') %>%
  mutate(Attested = ifelse(
    OverallFreq == 0, 0, 1
  )) %>%
  select(participant, resp, Word1, Word2, Alpha, Nonalpha, OverallFreq, Word1_freq, Word2_freq, Form, Percept, Culture, Power, Intense, Icon, Freq, Len, Lapse, "*BStress", RelFreq, GenPref, Attested) %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5)

all_human_data = all_human_data %>%
  mutate(resp_binary = case_when(
    resp == 'alpha' ~ 1,
    resp == 'nonalpha' ~ 0
  ))

all_human_data_novel    = all_human_data %>% filter(Attested == 0)
all_human_data_attested = all_human_data %>% filter(Attested == 1)



options(contrasts = c("contr.sum","contr.sum"))

corpus = read_csv('../Data/nonce_and_attested_binoms.csv')
model_preds_data = read_csv("../Data/grid_search_results.csv")
all_model_preds = model_preds_data
#all_model_preds = all_model_preds %>% filter(model %in% unique(all_model_preds$model)[1:2])

```



## functions


```{r}

item_base_fit  = NULL
human_base_fit = NULL


extract_slope = function(res, which = c("attested","novel")) {

  which = match.arg(which)

  fit =
    if (which == "attested") res$fit_human_attested
    else                     res$fit_human_novel

  # Skip failed runs
  if (is.null(fit) || !is.null(res$error) && res$error) return(NULL)

  # ----------------------------
  # brms backend
  # ----------------------------
  if (inherits(fit, "brmsfit")) {

    fe = fixef(fit)

    slope_row = grep("item_logodds", rownames(fe), value = TRUE)

    return(tibble(
      backend  = "brms",
      term     = slope_row,
      estimate = fe[slope_row, "Estimate"],
      se       = fe[slope_row, "Est.Error"],
      lo       = fe[slope_row, "Q2.5"],
      hi       = fe[slope_row, "Q97.5"]
    ))
  }

  # ----------------------------
  # lmer backend
  # ----------------------------
  if (inherits(fit, "glmerMod")) {

    fe = summary(fit)$coefficients

    slope_row = grep("item_logodds", rownames(fe), value = TRUE)

    return(tibble(
      backend  = "lmer",
      term     = slope_row,
      estimate = fe[slope_row, "Estimate"],
      se       = fe[slope_row, "Std. Error"],
      zvalue   = fe[slope_row, "z value"]
    ))
  }

  stop("Unknown model class")
}

# ----------------------------
# ITEM MODEL ADAPTER
# ----------------------------
fit_item_model = function(formula, data, backend, save_file=NULL) {

  if (backend == "brms") {

    if (is.null(item_base_fit)) {

      message("Compiling ITEM model...")

      item_base_fit <<- brm(
        formula,
        data   = data,
        prior  = c(
          prior(normal(0, 1), class = "Intercept"),
          prior(student_t(3, 0, 1), class = "sd"),
          prior(student_t(3, 0, 1), class = "sigma")
        ),
        iter   = 12000,
        warmup = 6000,
        chains = 4,
        cores  = 4,
        file   = save_file
      )

      return(item_base_fit)

    } else {

      message("Re-using compiled ITEM model via update()...")

      return(update(
        item_base_fit,
        newdata = data,
        cores = 4,
        chains = 4,
        file = save_file
      ))
    }

  } else if (backend == "lmer") {
    lmer(formula, data=data, REML=FALSE)
  } else stop("backend must be 'brms' or 'lmer'")
}



# ----------------------------
# HUMAN MODEL ADAPTER
# ----------------------------
fit_human_model = function(formula, data, backend, save_file=NULL) {

  if (backend == "brms") {

    if (is.null(human_base_fit)) {

      message("Compiling HUMAN model...")

      human_base_fit <<- brm(
        formula,
        data      = data,
        family    = bernoulli(),
        prior     = prior(normal(0, 1), class="b"),
        iter      = 6000,
        warmup    = 3000,
        chains    = 4,
        cores     = 4,
        save_pars = save_pars(all = TRUE),
        file      = save_file
      ) #%>% add_criterion("loo")

      return(human_base_fit)

    } else {

      message("Re-using compiled HUMAN model via update()...")

      return(
        update(
          human_base_fit,
          newdata = data,
          file = save_file,
          cores = 4,
          chains = 4
        ) #%>% add_criterion("loo")
      )
    }

  } else if (backend == "lmer") {

    glmer(
      formula,
      data   = data,
      family = binomial,
      control = glmerControl(optimizer="bobyqa"),
      nAGQ = 1
    )

  } else stop("backend must be 'brms' or 'lmer'")
}


extract_item_preds_backend = function(fit, alphas, prefix, backend) {

  alphas_chr = unique(as.character(alphas))

  newdata = data.frame(
    Alpha  = alphas_chr,
    prompt = NA_character_
  )

  if (backend == "lmer") {

    preds = predict(
      fit,
      newdata = newdata,
      re.form = ~(1 | Alpha),
      allow.new.levels = TRUE
    )

    tibble(
      Alpha = newdata$Alpha,
      !!paste0("item_logodds_", prefix) := preds
    )

  } else {

    fit_mat = fitted(
      fit,
      newdata          = newdata,
      re_formula       = ~(1 | Alpha),
      summary          = TRUE,
      allow_new_levels = TRUE
    )

    tibble(
      Alpha = newdata$Alpha,
      !!paste0("item_logodds_", prefix) := fit_mat[, "Estimate"],
      !!paste0("item_lo_", prefix)      := fit_mat[, "Q2.5"],
      !!paste0("item_hi_", prefix)      := fit_mat[, "Q97.5"]
    )
  }
}


is_handcoded = function(model_name) {
  grepl("handcoded", model_name, ignore.case = TRUE)
}
run_pythia_pipeline = function(
  model_name,
  all_model_preds,
  corpus,
  all_human_data_novel,
  all_human_data_attested,
  test_df,
  save_stub,
  backend = "brms",
  force_handcoded = FALSE   # üëà ADD THIS
) {

  message("Running pipeline for: ", model_name)

  handcoded_flag = force_handcoded || is_handcoded(model_name)

  model_df =
    all_model_preds %>%
    filter(model == model_name) %>%
    filter(prompt == target_prompt) %>%  
    mutate(log_odds = preference) %>%
    separate(binom, c("Word1","and","Word2"), remove=FALSE, sep=" ") %>%
    select(-and) %>%
    left_join(corpus, by=c("Word1","Word2")) %>%
    mutate(
      y_vals =
        0.02191943 + 0.23925834*Form +  0.24889543*Percept +
        0.41836997*Culture + 0.25967334*Power +
        0.01867604*Intense + 1.30365980*Icon +
        0.08553552*Freq + 0.15241566*Len -
        0.19381657*Lapse + 0.36019221*`*BStress`,
      GenPref = plogis(y_vals) - 0.5,
      RelFreq = RelFreq - 0.5
    ) %>%
    ungroup()

  model_novel   = filter(model_df, Attested == 0)
  model_attested =
    if (handcoded_flag) filter(model_df, Alpha %in% test_df$Alpha)
    else                filter(model_df, Attested == 1)

  item_novel <- model_novel %>%
  select(Alpha, item_logodds_singleprompt = log_odds)

  item_att <- model_attested %>%
    select(Alpha, item_logodds_singleprompt = log_odds)

  prefix = if (handcoded_flag) "pythia_handcoded" else "pythia_genpref"
  slope_var <- "item_logodds_singleprompt"

  human_novel <-
    all_human_data_novel %>%
    semi_join(item_novel, by="Alpha") %>%
    left_join(item_novel, by="Alpha")
  
  human_att <-
    (if (handcoded_flag)
       all_human_data_attested %>% semi_join(test_df, by="Alpha")
     else
       all_human_data_attested %>% semi_join(item_att, by="Alpha")
    ) %>%
    left_join(item_att, by="Alpha")

  human_formula = as.formula(
  paste0("resp_binary ~ ",
         slope_var,
         " + (1|participant) + (1|Alpha)")
)


  # ---------------------------
  # HUMAN MODELS
  # ---------------------------
  fit_human_novel =
    fit_human_model(human_formula, human_novel, backend,
                    save_file = paste0(save_stub,"_human_novel"))

  fit_human_att =
    fit_human_model(human_formula, human_att, backend,
                    save_file = paste0(save_stub,"_human_att"))

  list(
    backend = backend,
    handcoded = handcoded_flag,
    item_novel = item_novel,
    item_attested = item_att,
    fit_human_novel = fit_human_novel,
    fit_human_attested = fit_human_att
  )
}

```


```{r}
model_list = all_model_preds %>%
  distinct(model) %>%
  pull(model)

#model_list = model_list[1:289]

make_stub = function(model_name) {
  clean = gsub("[^A-Za-z0-9]+", "_", model_name)
  file.path("../Data", clean)
}

backend_choice = "lmer"   # "lmer" or "brms"

all_results_simplified = vector("list", length(model_list))
names(all_results_simplified) = model_list

for (m in model_list) {

  message("--------------")
  message(" Running: ", m)
  message("--------------")

  stub = make_stub(m)

  all_results_simplified[[m]] =
    tryCatch(
      run_pythia_pipeline(
        model_name = m,
        all_model_preds = all_model_preds,
        corpus = corpus,
        all_human_data_novel = all_human_data_novel,
        all_human_data_attested = all_human_data_attested,
        test_df = test_df,
        save_stub = stub,
        backend = backend_choice
      ),
      error = function(e) {
        message("‚ùå ERROR in ", m, ": ", e$message)
        list(error = TRUE)
      }
    )
}


baseline_models <- c(
  "EleutherAI/pythia-70m",
  "EleutherAI/pythia-160m",
  "EleutherAI/pythia-410m"
)

for (m in baseline_models) {

  message("===============")
  message(" BASELINE (handcoded subset): ", m)
  message("===============")

  stub_hc <- paste0(make_stub(m), "_handcoded_subset")

  all_results_simplified[[paste0(m, "__handcoded")]] <-
    tryCatch(
      run_pythia_pipeline(
        model_name = m,
        all_model_preds = all_model_preds,
        corpus = corpus,
        all_human_data_novel = all_human_data_novel,
        all_human_data_attested = all_human_data_attested,
        test_df = test_df,
        save_stub = stub_hc,
        backend = backend_choice,
        force_handcoded = TRUE     # üëà THIS IS THE ONLY DIFFERENCE
      ),
      error = function(e) {
        message("‚ùå ERROR in baseline handcoded ", m, ": ", e$message)
        list(error = TRUE)
      }
    )
}





coef_table_attested_simplified = map_df(names(all_results_simplified), function(m) {
  res = all_results_simplified[[m]]
  out = extract_slope(res, which = "attested")
  if (is.null(out)) return(NULL)
  out$model = m
  out
})

coef_table_novel_simplified = map_df(names(all_results_simplified), function(m) {
  res = all_results_simplified[[m]]
  out = extract_slope(res, which = "novel")
  if (is.null(out)) return(NULL)
  out$model = m
  out
})




best_attested_simplified = coef_table_attested_simplified %>%
  arrange(desc(estimate)) %>%
  slice(1)

best_novel_simplified = coef_table_novel_simplified %>%
  arrange(desc(estimate)) %>%
  slice(1)

write_csv(coef_table_attested_simplified, '../Data/coef_table_attested_lmer_simplified.csv')
write_csv(coef_table_novel_simplified, '../Data/coef_table_novel_lmer_simplified.csv')

```

```{r}

coef_aug_att <- coef_table_attested_simplified %>%
  mutate(
    # MODEL SIZE from either prefix or anywhere in name
    model_size = case_when(
      str_detect(model, regex("70m",  ignore_case=TRUE))  ~ "70m",
      str_detect(model, regex("160m", ignore_case=TRUE)) ~ "160m",
      str_detect(model, regex("410m", ignore_case=TRUE)) ~ "410m",
      TRUE ~ NA_character_
    ),

    # HANDCODED FLAG ‚Äî ANY mention of "handcoded"
    handcoded_flag =
      str_detect(model, regex("handcoded", ignore_case=TRUE)) |
      str_detect(model, "__handcoded$")
  )


baseline_rows_att <- coef_aug_att %>%
  filter(str_detect(model, "^EleutherAI/pythia")) %>%
  select(model_size, handcoded_flag,
         baseline_estimate = estimate)

coef_with_improvement_att <- coef_aug_att %>%
  left_join(
    baseline_rows_att,
    by = c("model_size", "handcoded_flag")
  ) %>%
  mutate(
    improvement = estimate - baseline_estimate
  )


coef_aug_novel <- coef_table_novel_simplified %>%
  mutate(
    model_size = case_when(
      str_detect(model, regex("70m",  ignore_case=TRUE))  ~ "70m",
      str_detect(model, regex("160m", ignore_case=TRUE)) ~ "160m",
      str_detect(model, regex("410m", ignore_case=TRUE)) ~ "410m",
      TRUE ~ NA_character_
    ),
    handcoded_flag =
      str_detect(model, regex("handcoded", ignore_case=TRUE)) |
      str_detect(model, "__handcoded$")
  )


baseline_rows_novel <- coef_aug_novel %>%
  filter(str_detect(model, "^EleutherAI/pythia"),
         handcoded_flag == FALSE) %>%   # << key change
  select(model_size,
         baseline_estimate = estimate)


coef_with_improvement_novel <- coef_aug_novel %>%
  left_join(
    baseline_rows_novel,
    by = c("model_size")
  ) %>%
  mutate(
    improvement = estimate - baseline_estimate
  )


```

