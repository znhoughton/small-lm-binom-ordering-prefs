---
title: "small-lm-analysis"
author: "Zachary Houghton"
date: "2025-07-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
```

# Prompt 1: "This next phrase must be formed compositionally: "

## Prep Data

Read in our model data

```{r}
corpus = read_csv('../Data/nonce_and_attested_binoms.csv')

model_smolm1 = read_csv('../Data/0-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2 = read_csv('../Data/0-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3 = read_csv('../Data/0-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1 = model_smolm1 %>%
  rename(no_final_stress = `*BStress`)

model_smolm2 = model_smolm2 %>%
  rename(no_final_stress = `*BStress`)

model_smolm3 = model_smolm3 %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attested = model_smolm1 %>%
  filter(Attested == 1)

model_smolm1_novel = model_smolm1 %>%
  filter(Attested == 0)

model_smolm2_attested = model_smolm2 %>%
  filter(Attested == 1)

model_smolm2_novel = model_smolm2 %>%
  filter(Attested == 0)

model_smolm3_attested = model_smolm3 %>%
  filter(Attested == 1)

model_smolm3_novel = model_smolm3 %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}
options(contrasts = c("contr.sum","contr.sum"))
prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)
model_smolm1_full_df = model_smolm1 
#model_smolm1_full_df$GenPref = scale(model_smolm1_full_df$GenPref)[,1]
#model_smolm1_full_df$RelFreq = scale(model_smolm1_full_df$RelFreq)[,1]
#model_smolm1_full_df$LogCenteredOverallFreq = scale(log(model_smolm1_full_df$OverallFreq + 0.000000001))[,1]


model_smolm1_full = brm(log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_full'
                      )

model_smolm1_full_df = model_smolm1
model_smolm1_full_df$LogOverallFreq = scale(log(model_smolm1_full_df$OverallFreq + 1))[,1]

model_smolm1_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_full_log_overall_freq_scaled'
                      )



model_smolm2_full_df = model_smolm2
model_smolm2_full_df$LogOverallFreq = scale(log(model_smolm2_full_df$OverallFreq + 1))[,1]

model_smolm2_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_full_log_overall_freq_scaled'
                      )



model_smolm3_full_df = model_smolm3
model_smolm3_full_df$LogOverallFreq = scale(log(model_smolm3_full_df$OverallFreq + 1))[,1]

model_smolm3_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_full_log_overall_freq_scaled'
                      )




fixef(model_smolm1_full_log_overall_freq_scaled)
fixef(model_smolm2_full_log_overall_freq_scaled)
fixef(model_smolm3_full_log_overall_freq_scaled)


model_smolm1_attested$LogOverallFreq = scale(log(model_smolm1_attested$OverallFreq))

model_smolm1_attested_model = brm( log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1_attested,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_model'
                      )


model_smolm2_attested$LogOverallFreq = scale(log(model_smolm2_attested$OverallFreq))

model_smolm2_attested_model = brm( log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2_attested,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_model'
                      )

model_smolm3_attested$LogOverallFreq = scale(log(model_smolm3_attested$OverallFreq))

model_smolm3_attested_model = brm( log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3_attested,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_model'
                      )


fixef(model_smolm1_attested_model)
fixef(model_smolm2_attested_model)
fixef(model_smolm3_attested_model)
```

### Novel Binomials

```{r}
options(contrasts = c("contr.sum","contr.sum"))
prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)


model_smolm1_novel_model = brm(log_odds ~ GenPref,
                       data = model_smolm1_novel,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_model'
                      )

model_smolm2_novel_model = brm(log_odds ~ GenPref,
                       data = model_smolm2_novel,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_model'
                      )

model_smolm3_novel_model = brm(log_odds ~ GenPref,
                       data = model_smolm3_novel,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_model'
                      )

fixef(model_smolm1_novel_model)
fixef(model_smolm2_novel_model)
fixef(model_smolm3_novel_model)



ce <- conditional_effects(model_smolm1_novel_model, effects = "GenPref",
                          re_formula = NA)   # fixed effects only
p = plot(ce, points = TRUE)
p[[1]] + theme_bw()



model_smolm1_novel_model_test = brm(log_odds ~ scale(GenPref),
                       data = model_smolm1_novel,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_model_test2'
                      )

fixef(model_smolm1_novel_model_test)
ce <- conditional_effects(model_smolm1_novel_model_test, effects = "GenPref",
                          re_formula = NA)   # fixed effects only
p = plot(ce, points = TRUE)
p[[1]] + 
  theme_bw() +
  xlab('Abstract Preferences') +
  ylab('Log Odds Alphabetical to Nonalphabetical') +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14)) 

```

# Prompt 2: "Next item: "

## Prep Data

Read in our model data

```{r}

model_smolm1b = read_csv('../Data/1-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2b = read_csv('../Data/1-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3b = read_csv('../Data/1-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1b = model_smolm1b %>%
  rename(no_final_stress = `*BStress`)

model_smolm2b = model_smolm2b %>%
  rename(no_final_stress = `*BStress`)

model_smolm3b = model_smolm3b %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attestedb = model_smolm1b %>%
  filter(Attested == 1)

model_smolm1_novelb = model_smolm1b %>%
  filter(Attested == 0)

model_smolm2_attestedb = model_smolm2b %>%
  filter(Attested == 1)

model_smolm2_novelb = model_smolm2b %>%
  filter(Attested == 0)

model_smolm3_attestedb = model_smolm3b %>%
  filter(Attested == 1)

model_smolm3_novelb = model_smolm3b %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}


model_smolm1b_full_df = model_smolm1b
model_smolm1b_full_df$LogOverallFreq = scale(log(model_smolm1b_full_df$OverallFreq + 1))[,1]

model_smolm1b_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1b_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1b_full_log_overall_freq_scaled'
                      )
fixef(model_smolm1b_full_log_overall_freq_scaled)

model_smolm2b_full_df = model_smolm2b
model_smolm2b_full_df$LogOverallFreq = scale(log(model_smolm2b_full_df$OverallFreq + 1))[,1]

model_smolm2b_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2b_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2b_full_log_overall_freq_scaled'
                      )
fixef(model_smolm2b_full_log_overall_freq_scaled)

model_smolm3b_full_df = model_smolm3b
model_smolm3b_full_df$LogOverallFreq = scale(log(model_smolm3b_full_df$OverallFreq + 1))[,1]

model_smolm3b_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3b_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3b_full_log_overall_freq_scaled'
                      )
fixef(model_smolm3b_full_log_overall_freq_scaled)

model_smolm1_attested_modelb = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_attestedb,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_modelb'
                      )

model_smolm2_attested_modelb = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm2_attestedb,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_modelb'
                      )


model_smolm3_attested_modelb = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm3_attestedb,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_modelb'
                      )


fixef(model_smolm1_attested_modelb)
fixef(model_smolm2_attested_modelb)
fixef(model_smolm3_attested_modelb)
```

### Novel Binomials

```{r}


model_smolm1_novel_modelb = brm(log_odds ~ GenPref,
                       data = model_smolm1_novelb,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_modelb'
                      )

model_smolm2_novel_modelb = brm(log_odds ~ GenPref,
                       data = model_smolm2_novelb,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_modelb'
                      )

model_smolm3_novel_modelb = brm(log_odds ~ GenPref,
                       data = model_smolm3_novelb,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_modelb'
                      )

fixef(model_smolm1_novel_modelb)
fixef(model_smolm2_novel_modelb)
fixef(model_smolm3_novel_modelb)
```


# Prompt 3: "example: "

## Prep Data

Read in our model data

```{r}

model_smolm1c = read_csv('../Data/2-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2c = read_csv('../Data/2-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3c = read_csv('../Data/2-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1c = model_smolm1c %>%
  rename(no_final_stress = `*BStress`)

model_smolm2c = model_smolm2c %>%
  rename(no_final_stress = `*BStress`)

model_smolm3c = model_smolm3c %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attestedc = model_smolm1c %>%
  filter(Attested == 1)

model_smolm1_novelc = model_smolm1c %>%
  filter(Attested == 0)

model_smolm2_attestedc = model_smolm2c %>%
  filter(Attested == 1)

model_smolm2_novelc = model_smolm2c %>%
  filter(Attested == 0)

model_smolm3_attestedc = model_smolm3c %>%
  filter(Attested == 1)

model_smolm3_novelc = model_smolm3c %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}

model_smolm1c_full_df = model_smolm1c
model_smolm1c_full_df$LogOverallFreq = scale(log(model_smolm1c_full_df$OverallFreq + 1))[,1]

model_smolm1c_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1c_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1c_full_log_overall_freq_scaled'
                      )
fixef(model_smolm1c_full_log_overall_freq_scaled)

model_smolm2c_full_df = model_smolm2c
model_smolm2c_full_df$LogOverallFreq = scale(log(model_smolm2c_full_df$OverallFreq + 1))[,1]

model_smolm2c_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2c_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2c_full_log_overall_freq_scaled'
                      )
fixef(model_smolm2c_full_log_overall_freq_scaled)

model_smolm3c_full_df = model_smolm3c
model_smolm3c_full_df$LogOverallFreq = scale(log(model_smolm3c_full_df$OverallFreq + 1))[,1]

model_smolm3c_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3c_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3c_full_log_overall_freq_scaled'
                      )
fixef(model_smolm3c_full_log_overall_freq_scaled)

model_smolm1_attested_modelc = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_attestedc,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_modelc'
                      )

model_smolm2_attested_modelc = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm2_attestedc,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_modelc'
                      )


model_smolm3_attested_modelc = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm3_attestedc,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_modelc'
                      )


fixef(model_smolm1_attested_modelc)
fixef(model_smolm2_attested_modelc)
fixef(model_smolm3_attested_modelc)
```

### Novel Binomials

```{r}


model_smolm1_novel_modelc = brm(log_odds ~ GenPref,
                       data = model_smolm1_novelc,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_modelc'
                      )

model_smolm2_novel_modelc = brm(log_odds ~ GenPref,
                       data = model_smolm2_novelc,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_modelc'
                      )

model_smolm3_novel_modelc = brm(log_odds ~ GenPref,
                       data = model_smolm3_novelc,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_modelc'
                      )

fixef(model_smolm1_novel_modelc)
fixef(model_smolm2_novel_modelc)
fixef(model_smolm3_novel_modelc)
```



# Prompt 4: "instance: "

## Prep Data

Read in our model data

```{r}

model_smolm1d = read_csv('../Data/3-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2d = read_csv('../Data/3-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3d = read_csv('../Data/3-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1d = model_smolm1d %>%
  rename(no_final_stress = `*BStress`)

model_smolm2d = model_smolm2d %>%
  rename(no_final_stress = `*BStress`)

model_smolm3d = model_smolm3d %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attestedd = model_smolm1d %>%
  filter(Attested == 1)

model_smolm1_noveld = model_smolm1d %>%
  filter(Attested == 0)

model_smolm2_attestedd = model_smolm2d %>%
  filter(Attested == 1)

model_smolm2_noveld = model_smolm2d %>%
  filter(Attested == 0)

model_smolm3_attestedd = model_smolm3d %>%
  filter(Attested == 1)

model_smolm3_noveld = model_smolm3d %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}

model_smolm1d_full_df = model_smolm1d
model_smolm1d_full_df$LogOverallFreq = scale(log(model_smolm1d_full_df$OverallFreq + 1))[,1]

model_smolm1d_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1d_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1d_full_log_overall_freq_scaled'
                      )

model_smolm2d_full_df = model_smolm2d
model_smolm2d_full_df$LogOverallFreq = scale(log(model_smolm2d_full_df$OverallFreq + 1))[,1]

model_smolm2d_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2d_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2d_full_log_overall_freq_scaled'
                      )



model_smolm3d_full_df = model_smolm3d
model_smolm3d_full_df$LogOverallFreq = scale(log(model_smolm3d_full_df$OverallFreq + 1))[,1]

model_smolm3d_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3d_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3d_full_log_overall_freq_scaled'
                      )

fixef(model_smolm3d_full_log_overall_freq_scaled)


model_smolm1_attested_modeld = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_attestedd,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_modeld'
                      )

model_smolm2_attested_modeld = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm2_attestedd,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_modeld'
                      )


model_smolm3_attested_modeld = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm3_attestedd,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_modeld'
                      )


fixef(model_smolm1_attested_modeld)
fixef(model_smolm2_attested_modeld)
fixef(model_smolm3_attested_modeld)
```

### Novel Binomials

```{r}


model_smolm1_novel_modeld = brm(log_odds ~ GenPref,
                       data = model_smolm1_noveld,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_modeld'
                      )

model_smolm2_novel_modeld = brm(log_odds ~ GenPref,
                       data = model_smolm2_noveld,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_modeld'
                      )

model_smolm3_novel_modeld = brm(log_odds ~ GenPref,
                       data = model_smolm3_noveld,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_modeld'
                      )

fixef(model_smolm1_novel_modeld)
fixef(model_smolm2_novel_modeld)
fixef(model_smolm3_novel_modeld)
```



# Prompt 5: "try this: "

## Prep Data

Read in our model data

```{r}

model_smolm1e = read_csv('../Data/4-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2e = read_csv('../Data/4-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3e = read_csv('../Data/4-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1e = model_smolm1e %>%
  rename(no_final_stress = `*BStress`)

model_smolm2e = model_smolm2e %>%
  rename(no_final_stress = `*BStress`)

model_smolm3e = model_smolm3e %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attestede = model_smolm1e %>%
  filter(Attested == 1)

model_smolm1_novele = model_smolm1e %>%
  filter(Attested == 0)

model_smolm2_attestede = model_smolm2e %>%
  filter(Attested == 1)

model_smolm2_novele = model_smolm2e %>%
  filter(Attested == 0)

model_smolm3_attestede = model_smolm3e %>%
  filter(Attested == 1)

model_smolm3_novele = model_smolm3e %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}
model_smolm1e_full_df = model_smolm1e
model_smolm1e_full_df$LogOverallFreq = scale(log(model_smolm1e_full_df$OverallFreq + 1))[,1]

model_smolm1e_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1e_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1e_full_log_overall_freq_scaled'
                      )
fixef(model_smolm1e_full_log_overall_freq_scaled)

model_smolm2e_full_df = model_smolm2e
model_smolm2e_full_df$LogOverallFreq = scale(log(model_smolm2e_full_df$OverallFreq + 1))[,1]

model_smolm2e_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2e_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2e_full_log_overall_freq_scaled'
                      )
fixef(model_smolm2e_full_log_overall_freq_scaled)


model_smolm3e_full_df = model_smolm3e
model_smolm3e_full_df$LogOverallFreq = scale(log(model_smolm3e_full_df$OverallFreq + 1))[,1]

model_smolm3e_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3e_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3e_full_log_overall_freq_scaled'
                      )
fixef(model_smolm3e_full_log_overall_freq_scaled)

model_smolm1_attested_modele = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_attestede,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_modele'
                      )

model_smolm2_attested_modele = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm2_attestede,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_modele'
                      )


model_smolm3_attested_modele = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm3_attestede,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_modele'
                      )


fixef(model_smolm1_attested_modele)
fixef(model_smolm2_attested_modele)
fixef(model_smolm3_attested_modele)
```

### Novel Binomials

```{r}


model_smolm1_novel_modele = brm(log_odds ~ GenPref,
                       data = model_smolm1_novele,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_modele'
                      )

model_smolm2_novel_modele = brm(log_odds ~ GenPref,
                       data = model_smolm2_novele,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_modele'
                      )

model_smolm3_novel_modele = brm(log_odds ~ GenPref,
                       data = model_smolm3_novele,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_modele'
                      )

fixef(model_smolm1_novel_modele)
fixef(model_smolm2_novel_modele)
fixef(model_smolm3_novel_modele)
```



# Prompt 6: "[BOS]"

## Prep Data

Read in our model data

```{r}

model_smolm1f = read_csv('../Data/5-smolm-autoreg-bpe-babylm-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  
model_smolm2f = read_csv('../Data/5-smolm-autoreg-bpe-babylm-seed_211-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form + 0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm3f = read_csv('../Data/5-smolm-autoreg-bpe-babylm-seed_1024-1e-3_binom_ordering_prefs.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = case_when(Attested == 1 ~ RelFreq - 0.5,
                             Attested == 0 ~ 0)) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)
  

model_smolm1f = model_smolm1f %>%
  rename(no_final_stress = `*BStress`)

model_smolm2f = model_smolm2f %>%
  rename(no_final_stress = `*BStress`)

model_smolm3f = model_smolm3f %>%
  rename(no_final_stress = `*BStress`)
```

```{r}

model_smolm1_attestedf = model_smolm1f %>%
  filter(Attested == 1)

model_smolm1_novelf = model_smolm1f %>%
  filter(Attested == 0)

model_smolm2_attestedf = model_smolm2f %>%
  filter(Attested == 1)

model_smolm2_novelf = model_smolm2f %>%
  filter(Attested == 0)

model_smolm3_attestedf = model_smolm3f %>%
  filter(Attested == 1)

model_smolm3_novelf = model_smolm3f %>%
  filter(Attested == 0)

#bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
#onegram_corpus_size = 1560674367427
#count_and = 43378012800

#bigram_counts = bigram_counts %>%
  #mutate(ngram = str_replace_all(ngram, '\t', ' '))
# 
# data_main_model = data_main_model %>%
#   mutate(bigram1_alpha = paste0(Word1, ' and'),
#          bigram2_alpha = paste0('and ', Word2),
#          bigram1_nonalpha = paste0(Word2, ' and'),
#          bigram2_nonalpha = paste0('and ', Word1)
#          )
# 
# data_main_model = data_main_model %>%
#   left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
#   rename(count_bigram1_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
#   rename(count_bigram2_alpha = count) %>%
#   left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
#   rename(count_bigram1_nonalpha = count) %>%
#   left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
#   rename(count_bigram2_nonalpha = count) %>%
#   mutate(corpus_size = onegram_corpus_size) %>%
#   mutate(count_and = count_and)

# 
# data_main_model = data_main_model %>%
#   mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
#          bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
#          log_bigram_prob_alpha = log(bigram_prob_alpha),
#          log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
#          log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)
#
```

## Analyses

### Attested Binomials

```{r}
model_smolm1f_full_df = model_smolm1f
model_smolm1f_full_df$LogOverallFreq = scale(log(model_smolm1f_full_df$OverallFreq + 1))[,1]

model_smolm1f_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm1f_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1f_full_log_overall_freq_scaled'
                      )

model_smolm2f_full_df = model_smolm2f
model_smolm2f_full_df$LogOverallFreq = scale(log(model_smolm2f_full_df$OverallFreq + 1))[,1]

model_smolm2f_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm2f_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2f_full_log_overall_freq_scaled'
                      )

model_smolm3f_full_df = model_smolm3f
model_smolm3f_full_df$LogOverallFreq = scale(log(model_smolm3f_full_df$OverallFreq + 1))[,1]

model_smolm3f_full_log_overall_freq_scaled = brm(log_odds ~ GenPref + RelFreq + GenPref:LogOverallFreq + RelFreq:LogOverallFreq + LogOverallFreq,
                       data = model_smolm3f_full_df,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3f_full_log_overall_freq_scaled'
                      )

fixef(model_smolm1f_full_log_overall_freq_scaled)
model_smolm1_attested_modelf = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm1_attestedf,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm1_attested_modelf'
                      )

model_smolm2_attested_modelf = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm2_attestedf,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm2_attested_modelf'
                      )


model_smolm3_attested_modelf = brm( log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       data = model_smolm3_attestedf,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 20),
                       file = '../Data/model_smolm3_attested_modelf'
                      )


fixef(model_smolm1_attested_modelf)
fixef(model_smolm2_attested_modelf)
fixef(model_smolm3_attested_modelf)
```

### Novel Binomials

```{r}


model_smolm1_novel_modelf = brm(log_odds ~ GenPref,
                       data = model_smolm1_novelf,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm1_novel_modelf'
                      )

model_smolm2_novel_modelf = brm(log_odds ~ GenPref,
                       data = model_smolm2_novelf,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm2_novel_modelf'
                      )

model_smolm3_novel_modelf = brm(log_odds ~ GenPref,
                       data = model_smolm3_novelf,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_smolm3_novel_modelf'
                      )

fixef(model_smolm1_novel_modelf)
fixef(model_smolm2_novel_modelf)
fixef(model_smolm3_novel_modelf)
```


# Qing's model, all prompts

```{r}
options(contrasts = c("contr.sum","contr.sum"))

corpus = read_csv('../Data/nonce_and_attested_binoms.csv')

baseline_model_data = read_csv('../Data/SMALL_MODEL_BINOMIAL_PREFERENCES.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5) %>%
  ungroup()


small_attested = read_csv('../Data/SMALL_MODEL_BINOMIAL_PREFERENCES.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5) %>%
  ungroup() %>%
  filter(Attested == 1) %>%
  mutate(CenteredOverallFreq = scale(log(OverallFreq))) 


small_novel = read_csv('../Data/SMALL_MODEL_BINOMIAL_PREFERENCES.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  filter(Attested == 0)

small_attested$prompt = factor(small_attested$prompt)
small_novel$prompt = factor(small_novel$prompt)

small_novel = small_novel %>%
  rename(no_final_stress = "*BStress")

small_attested = small_attested %>%
  rename(no_final_stress = "*BStress")

model_small_attested_genpref = brm(log_odds ~ GenPref + RelFreq + CenteredOverallFreq + GenPref:CenteredOverallFreq + RelFreq:CenteredOverallFreq + (GenPref | prompt) + (1 | Alpha),
                       data = small_attested,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/model_small_attested_genpref'
                      )


model_small_novel_genpref = brm(log_odds ~ GenPref + (GenPref | prompt) + (1 | Alpha),
                       data = small_novel,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_small_novel_genpref'
                      )


### Vif then backward model selection

model_small_novel_individual_constraints = brm(log_odds ~ Culture + Power + Freq + Len + no_final_stress + Intense + Percept + Lapse + (Culture + Power + Freq + Len + no_final_stress + Intense + Percept + Lapse | prompt) + ( 1 | Alpha),
                       data = small_novel,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_small_novel_individual_constraints'
                      )
 

fixef(model_small_attested_genpref)
fixef(model_small_novel_genpref)
fixef(model_small_novel_individual_constraints)
#car::vif(model_small_novel_individual_constraints)


## Now we do backward model selection? For this project we might actually want to *not* do backward model selection, because constraints that are centered around zero may change after finetuning.


```

# Finetuning on GenPref



```{r}
options(contrasts = c("contr.sum","contr.sum"))

corpus = read_csv('../Data/nonce_and_attested_binoms.csv')

model_results_genpref = read_csv('../Data/FINETUNED_GENPREF_MODEL_BINOMIAL_PREFERENCES.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5) %>%
  ungroup() 


model_results_genpref$prompt = factor(model_results_genpref$prompt)

model_results_genpref = model_results_genpref %>%
  rename(no_final_stress = "*BStress")



small_attested_genpref = model_results_genpref %>%
  filter(Attested == 1) %>%
  mutate(CenteredOverallFreq = scale(log(OverallFreq))) 

small_novel_genpref = model_results_genpref %>%
  filter(Attested == 0)

small_attested_genpref$prompt = factor(small_attested_genpref$prompt)
small_novel_genpref$prompt = factor(small_novel_genpref$prompt)
```

```{r}

######################################################################################
######################################################################################
## we'll come back to running these models once we have found the best language model 
######################################################################################
######################################################################################


model_small_attested_genpref_finetuned = brm(log_odds ~ (GenPref + RelFreq + CenteredOverallFreq + GenPref:CenteredOverallFreq + RelFreq:CenteredOverallFreq) * model + (GenPref | prompt) + (1 | Alpha),
                       data = small_attested_genpref,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/model_small_attested_genpref_finetuned'
                      )


model_small_novel_genpref_finetuned = brm(log_odds ~ GenPref * model + (GenPref * model | prompt) + (1 | Alpha),
                       data = small_novel_genpref,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_small_novel_genpref_finetuned'
                      )


### Vif then backward model selection

model_small_novel_individual_constraints_finetuned = brm(log_odds ~ Culture + Power + Freq + Len + no_final_stress + Intense + Percept + Lapse + (Culture + Power + Freq + Len + no_final_stress + Intense + Percept + Lapse | prompt) + ( 1 | Alpha),
                       data = small_novel_genpref,
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_small_novel_individual_constraints_finetuned'
                      )
 

fixef(model_small_attested_genpref_finetuned)
fixef(model_small_novel_genpref_finetuned)
fixef(model_small_novel_individual_constraints)
#car::vif(model_small_novel_individual_constraints_finetuned)


library(dplyr)
library(tidyr)

df_item_model <- model_results_genpref %>%
  full_join(baseline_model_data) #%>%
  group_by(Alpha, model) %>%
  summarise(
    log_odds = mean(log_odds, na.rm = TRUE),
    .groups = "drop"
  )

library(tidyr)

df_wide <- df_item_model %>%
  pivot_wider(
    names_from  = model,
    values_from = log_odds
  )


cor_mat <- df_wide %>%
  select(-Alpha) %>%
  cor(use = "pairwise.complete.obs")

```

```{r}

# all_human_data = nonce_data
all_human_data = read_csv('../Human Data and Analyses/all_human_data.csv') %>%
  mutate(Attested = ifelse(
    OverallFreq == 0, 0, 1
  )) %>%
  select(participant, resp, Word1, Word2, Alpha, Nonalpha, OverallFreq, Word1_freq, Word2_freq, Form, Percept, Culture, Power, Intense, Icon, Freq, Len, Lapse, "*BStress", RelFreq, GenPref, Attested) %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5)

all_human_data = all_human_data %>%
  mutate(resp_binary = case_when(
    resp == 'alpha' ~ 1,
    resp == 'nonalpha' ~ 0
  ))


#   filter(Attested == 0) 
# 
# small_novel_genpref_item_preds = brm(log_odds ~ 1 + (1 | Alpha) + (1 | prompt),
#                        data = small_novel_genpref,
#                        #prior = prior_probs,
#                        iter = 6000,
#                        warmup = 3000,
#                        chains = 4,
#                        cores = 4,
#                        #control = list(adapt_delta=0.99, max_treedepth = 15),
#                        #control = list(max_treedepth = 20),
#                        file = '../Data/small_novel_genpref_item_preds'
#                       )

prior_probs <- c(
  prior(normal(0, 1), class = "Intercept"),
  prior(student_t(3, 0, 1), class = "sd", group = "Alpha"),
  prior(student_t(3, 0, 1), class = "sd", group = "prompt"),
  prior(student_t(3, 0, 1), class = "sigma")
)

#now let's sample 100 binomials/items
item_list = unique(baseline_model_data$Alpha)
#set.seed(42)
sampled_items = item_list #we can replace this with simply "sampled_items = item_list" later when we want to run the full thing

baseline_model_data = baseline_model_data %>% 
  filter(Alpha %in% sampled_items) 

baseline_item_preds = brm(log_odds ~ 1 + (1 | Alpha) + (1 | prompt),
                       data = baseline_model_data,
                       prior = prior_probs,
                       iter =  10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/baseline_item_preds'
                      )


newdata = data.frame(
  Alpha = unique(baseline_model_data$Alpha),
  prompt = NA
)

# fitA = fitted(
#   small_novel_genpref_item_preds,
#   newdata = newdata,
#   re_formula = ~ (1 | Alpha),
#   summary = TRUE
# )
# 
# item_df_genpref = data.frame(
#   Alpha = newdata$Alpha,
#   item_logodds_genpref = fitA[, "Estimate"],
#   item_lo_genpref      = fitA[, "Q2.5"],
#   item_hi_genpref      = fitA[, "Q97.5"]
# )

fitB = fitted(
  baseline_item_preds,
  newdata = newdata,
  re_formula = ~(1 | Alpha),
  summary = TRUE
)

item_df_baseline = data.frame(
  Alpha = newdata$Alpha,
  item_logodds_baseline = fitB[, "Estimate"],
  item_lo_baseline      = fitB[, "Q2.5"],
  item_hi_baseline      = fitB[, "Q97.5"]
)




human_data = all_human_data %>%
  left_join(item_df_baseline, by = 'Alpha') %>%
  filter(Alpha %in% sampled_items)

test = human_data %>%
  filter(is.na(resp))

nrow(test)
nrow(human_data)

prior_probs2 <- c(
  # Fixed effects
  prior(normal(0, 1), class = "Intercept"),
  prior(normal(0, 1), class = "b", coef = "item_logodds_baseline"),

  # Random-effect standard deviations
  prior(student_t(3, 0, 1), class = "sd", group = "participant"),
  prior(student_t(3, 0, 1), class = "sd", group = "Alpha")
)


human_data_baseline = brm(
  resp_binary ~ item_logodds_baseline + (1 | participant) + (1 | Alpha),
  data = human_data,
  family = bernoulli(link = "logit"),
  chains = 4,   
  cores = 4,
  iter = 6000,  
  warmup = 3000,
  save_pars = save_pars(all = TRUE),
  prior = prior_probs2,
  #control = list(adapt_delta=0.99, max_treedepth = 20),
  file = '../Data/human_data_baseline'
)
# 
# human_data_genpref = brm(
#   resp_binary ~ item_logodds_genpref + (1 | participant) + (1 | Alpha),
#   data = human_data,
#   family = bernoulli(link = "logit"),
#   chains = 4,   
#   cores = 4,
#   iter = 6000,  
#   warmup = 3000,
#   save_pars = save_pars(all = TRUE),
#   #prior = prior_probs,
#   #control = list(adapt_delta=0.99, max_treedepth = 20),
#   file = '../Data/human_data_genpref'
# )

human_data_baseline = add_criterion(human_data_baseline, 'loo')
#human_data_genpref = add_criterion(human_data_genpref, 'loo')

# loo_results = data.frame(loo_compare(
#   human_data_baseline,
#   human_data_genpref
# ))

#loo_results

```

## Testing multiple genpref models

```{r}
# 
# template_item_model <- brm(
#   log_odds ~ 1 + (1 | Alpha) + (1 | prompt),
#   data = df_genpref,   # any representative dataset
#   prior = prior_probs,
#   iter = 1000,
#   warmup = 5000,
#   chains = 4,
#   cores = 4,
#   backend = "cmdstanr",
#   file = "../Data/template_genpref_item_model"
# )
# 
# template_human_model <- brm(
#   resp_binary ~ item_logodds_genpref + (1 | participant) + (1 | Alpha),
#   data = human_data_model,
#   family = bernoulli(link = "logit"),
#   prior = prior_probs2,
#   chains = 4,
#   cores = 4,
#   iter = 6000,
#   warmup = 3000,
#   backend = "cmdstanr",
#   save_pars = save_pars(all = TRUE),
#   file = "../Data/template_human_model"
# )


library(purrr)

run_genpref_pipeline <- function(df_genpref, model_name) {

  message("Running model: ", model_name)
  
  df_genpref = df_genpref %>%
    filter(Alpha %in% sampled_items)
  # --------------------------------------------------
  # Item-level model
  # --------------------------------------------------
  safe_model_name <- gsub("[^A-Za-z0-9_]", "_", model_name)
  item_model <- update(
    baseline_item_preds,
    newdata = df_genpref,
    recompile = FALSE,
    chains = 4,
    cores = 4,
    file = paste0("../Data/genpref_item_preds_", safe_model_name)
  )


  newdata <- data.frame(
    Alpha = unique(df_genpref$Alpha),
    prompt = NA
  )

  fitA <- fitted(
    item_model,
    newdata = newdata,
    re_formula = ~ (1 | Alpha),
    summary = TRUE
  )

  item_df_genpref <- data.frame(
    Alpha = newdata$Alpha,
    item_logodds_genpref = fitA[, "Estimate"],
    item_lo_genpref      = fitA[, "Q2.5"],
    item_hi_genpref      = fitA[, "Q97.5"]
  )

  # --------------------------------------------------
  # Join with human data
  # --------------------------------------------------
  human_data_model <- all_human_data %>%
    left_join(item_df_baseline, by = "Alpha") %>%
    left_join(item_df_genpref, by = "Alpha") %>%
    filter(Alpha %in% sampled_items)


  # --------------------------------------------------
  # Human-level model
  # --------------------------------------------------
  human_model <- update(
    human_data_baseline,
    newdata = human_data_model,
    recompile = FALSE,
    chains = 4, 
    cores = 4,
    file = paste0("../Data/human_data_genpref_", safe_model_name)
  )


  human_model <- add_criterion(human_model, "loo")
 
   
  col_name <- paste0("item_logodds_genpref_", safe_model_name)

  human_data_model <- human_data_model %>%
    rename(!!col_name := item_logodds_genpref)

  # --------------------------------------------------
  # LOO comparison
  # --------------------------------------------------
  loo_df <- data.frame(
    loo_compare(
      human_data_baseline,
      human_model
    )
  )

  loo_df$model <- model_name
  list(
  model = model_name,
  loo = loo_df,
  human_data = human_data_model %>%
    select(
      participant, resp, Word1, Word2, Alpha, Nonalpha,
      OverallFreq, Word1_freq, Word2_freq,
      Form, Percept, Culture, Power, Intense, Icon,
      Freq, Len, Lapse, "*BStress", RelFreq, GenPref, Attested,
      item_logodds_baseline,
      all_of(col_name),
      resp_binary
    )
)

}

results <- model_results_genpref %>%
  split(.$model) %>%
  imap(~ run_genpref_pipeline(.x, .y))


loo_results_all <- bind_rows(map(results, "loo"))
loo_results_all


human_data_wide <- reduce(
  map(results, "human_data"),
  left_join,
  by = c(
    "participant", "resp", "Word1", "Word2", "Alpha", "Nonalpha",
    "OverallFreq", "Word1_freq", "Word2_freq",
    "Form", "Percept", "Culture", "Power", "Intense", "Icon",
    "Freq", "Len", "Lapse", "*BStress", "RelFreq", "GenPref", "Attested",
    "item_logodds_baseline", "resp_binary"
  )
)

```

```{r}

human_model = brm(
  resp_binary ~ item_logodds_baseline + (1 | participant) + (1 | Alpha),
  data = human_data,
  family = bernoulli(link = "logit"),
  chains = 4,   
  cores = 4,
  iter = 6000,  
  warmup = 3000,
  save_pars = save_pars(all = TRUE),
  prior = prior_probs2,
  #control = list(adapt_delta=0.99, max_treedepth = 20),
  file = '../Data/human_data_genpref_qing_yao_genfreq_finetuned_ep10_seed_42_1e_4.rds'
)

test_loo = data.frame(
    loo_compare(
      human_data_baseline,
      human_model
    )
  )
```


## Expansive Analyses now that we have selected a model

